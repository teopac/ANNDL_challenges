{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"notebook da consegnare","provenance":[{"file_id":"1ZnWtYfOGBKJszsRJHfugfZ5xXMq7q_Es","timestamp":1611686246280},{"file_id":"1q7Q551gJnlUV6XZnk1xgbYsC_wIge7bD","timestamp":1611674657255},{"file_id":"15WQdHJ9fZCZa46qbvMJR_gVF1iOfbDra","timestamp":1611656424127},{"file_id":"1IE6SSYDDeL7ky5JH7WxA_vt-Xo0NJjMF","timestamp":1611652701655}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"80N8bdR2XVW-"},"source":["# VGG - Glove 300d model\n","This model implements the feature extraction with a CNN based on VGG while the RNN is based on a single LSTM block. In the end the features extracted and the output of the LSTM are concatenated into a dense layer. During the training procedure we faced overfitting using:\n","* Dropout: 0.3 after the dense layer\n","* Weigth Decay: l2-norm with 1e-5\n","* Early Stopping: with patience 5\n","\n","Images are resized to 512x256 and we used a batch of size 64 while the learning rate was 1e-3."]},{"cell_type":"code","metadata":{"id":"X5dMD6GANQbB"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joIbWTdJNbKf"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NK-n9igbgeLp"},"source":["## Model hyperparameters"]},{"cell_type":"code","metadata":{"id":"LZd9P_UxNhdB"},"source":["SEED = 1234\n","img_w = 512\n","img_h = 256\n","bs = 64\n","lr = 1e-3\n","num_epochs = 100\n","patience = 5\n","\n","# kfold\n","val_split_perc = 0.2\n","k = 5\n","enable_kfold = True\n","if not enable_kfold:\n","  k = int(1 / val_split_perc)\n","\n","model_name = '512x256_noWeight_gloveTrainable300d'\n","\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)   \n","\n","MAX_NUM_SENTENCES = 100000\n","MAX_NUM_WORDS = 999888777666"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDvzaIF8N1Q-"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ida5bxMg7Ib"},"source":["## Prepare Dataset\n","First we get the Glove embedding with the 300d size and then we unzip the vqa dataset of the challenge."]},{"cell_type":"code","metadata":{"id":"36etqnHawmbM"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n","zip_path = \"/content/glove.6B.zip\"\r\n","if not os.path.exists(\"/content/glove.6B.300d.txt\"):\r\n","  !unzip  -q {zip_path}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K764SUYezQV8"},"source":["embeddings_index = {}\r\n","embedding_dim = 300\r\n","with open(\"/content/glove.6B.300d.txt\") as f:\r\n","    for line in f:\r\n","        word, coefs = line.split(maxsplit=1)\r\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n","        embeddings_index[word] = coefs\r\n","\r\n","print(\"Found %s word vectors.\" % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xzd4LAtN46i"},"source":["zip_path = \"/content/drive/MyDrive/challenge3/dataset/anndl-2020-vqa.zip\"\n","cwd = os.getcwd()\n","if not os.path.exists(os.path.join(cwd, \"VQA_Dataset\")):\n","  !unzip  -q {zip_path}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHBbc5cLhUw_"},"source":["### Dataset Initialization"]},{"cell_type":"code","metadata":{"id":"wi6NYEzmQUmV"},"source":["import pandas as pd \n","import json\n","# reading the JSON data using json.load()\n","file_path = \"/content/VQA_Dataset/train_questions_annotations.json\"\n","with open(file_path) as train_file:\n","    dict_dataset = json.load(train_file)\n","    dataset_frame = pd.DataFrame.from_dict(dict_dataset, orient='index', dtype=(\"str\", \"str\"))\n","    dataset_frame.reset_index(level=0, inplace=True)\n","    #mischia il dataset\n","    dataset_frame=dataset_frame.iloc[np.random.permutation(dataset_frame.index)].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrbyKsBhCmdI"},"source":["test_file_path = \"/content/VQA_Dataset/test_questions.json\"\r\n","with open(test_file_path) as test_file:\r\n","    test_dict_dataset = json.load(test_file)\r\n","    test_dataset_frame = pd.DataFrame.from_dict(test_dict_dataset, orient='index', dtype=(\"str\", \"str\"))\r\n","    test_dataset_frame.reset_index(level=0, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"de7-FkoHs5x1"},"source":["Map the answers to the classes."]},{"cell_type":"code","metadata":{"id":"yIlnvHPfcXGX"},"source":["#copied from kaggle\n","labels_dict = {\n","        '0': 0,\n","        '1': 1,\n","        '2': 2,\n","        '3': 3,\n","        '4': 4,\n","        '5': 5,\n","        'apple': 6,\n","        'baseball': 7,\n","        'bench': 8,\n","        'bike': 9,\n","        'bird': 10,\n","        'black': 11,\n","        'blanket': 12,\n","        'blue': 13,\n","        'bone': 14,\n","        'book': 15,\n","        'boy': 16,\n","        'brown': 17,\n","        'cat': 18,\n","        'chair': 19,\n","        'couch': 20,\n","        'dog': 21,\n","        'floor': 22,\n","        'food': 23,\n","        'football': 24,\n","        'girl': 25,\n","        'grass': 26,\n","        'gray': 27,\n","        'green': 28,\n","        'left': 29,\n","        'log': 30,\n","        'man': 31,\n","        'monkey bars': 32,\n","        'no': 33,\n","        'nothing': 34,\n","        'orange': 35,\n","        'pie': 36,\n","        'plant': 37,\n","        'playing': 38,\n","        'red': 39,\n","        'right': 40,\n","        'rug': 41,\n","        'sandbox': 42,\n","        'sitting': 43,\n","        'sleeping': 44,\n","        'soccer': 45,\n","        'squirrel': 46,\n","        'standing': 47,\n","        'stool': 48,\n","        'sunny': 49,\n","        'table': 50,\n","        'tree': 51,\n","        'watermelon': 52,\n","        'white': 53,\n","        'wine': 54,\n","        'woman': 55,\n","        'yellow': 56,\n","        'yes': 57\n","}\n","num_classes = len(labels_dict.keys())\n","dataset_frame[\"answer\"] = [labels_dict[x] for x in dataset_frame[\"answer\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77YvFnnqByDO"},"source":["occurrences_dict = {}\r\n","for c in np.unique(dataset_frame[\"answer\"]):\r\n","  unique, count = np.unique(dataset_frame[\"answer\"] == c, return_counts=True)\r\n","  occurrences_dict[c] = count[1]\r\n","\r\n","#occurrences_dict\r\n","sum = 0\r\n","for v in occurrences_dict.values():\r\n"," sum+=v\r\n","\r\n","sum\r\n","print(\"\\n\")\r\n","len(dataset_frame)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqaSLzgvE6ak"},"source":["weight_dict = {}\r\n","for k in occurrences_dict.keys():\r\n","  weight_dict[k] = (1/occurrences_dict[k])*(len(dataset_frame))/float(num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8JSxSM9ug2l"},"source":["## Word processing"]},{"cell_type":"code","metadata":{"id":"_PQpR6_Bc4TK"},"source":["def add_eos(string):\n","  return string + \" <eos>\"\n","\n","#aggiungo l'eos \n","dataset_frame[\"question\"] = dataset_frame[\"question\"].map(lambda question : question.replace(\"?\" , \"\"))\n","dataset_frame[\"question\"] = dataset_frame[\"question\"].map(lambda question : question + \" <eos>\")\n","\n","#la max len può essere un qualsiasi numero. Qua considero la  frase più lunga che ho\n","MAX_LEN = max([len(x.split(\" \")) for x in dataset_frame[\"question\"]]) \n","print(\"MAX_LEN={}\".format(MAX_LEN))\n","\n","#riduco il numero di frasi da considerare\n","dataset_frame = dataset_frame[:MAX_NUM_SENTENCES]\n","print(len(dataset_frame))\n","#scarto le frasi troppo lunghe\n","condition = [len(x.split(\" \")) <= MAX_LEN  for x in dataset_frame[\"question\"]]\n","dataset_frame  = dataset_frame[condition]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgMbwCmMEtxZ"},"source":["#aggiungo l'eos \r\n","test_dataset_frame[\"question\"] = test_dataset_frame[\"question\"].map(lambda question : question.replace(\"?\" , \"\"))\r\n","test_dataset_frame[\"question\"] = test_dataset_frame[\"question\"].map(lambda question : question + \" <eos>\")\r\n","\r\n","#la max len può essere un qualsiasi numero. Qua considero la  frase più lunga che ho\r\n","TEST_MAX_LEN = max([len(x.split(\" \")) for x in test_dataset_frame[\"question\"]]) \r\n","print(\"TEST_MAX_LEN={}\".format(TEST_MAX_LEN))\r\n","\r\n","#riduco il numero di frasi da considerare\r\n","test_dataset_frame = test_dataset_frame[:MAX_NUM_SENTENCES]\r\n","print(len(test_dataset_frame))\r\n","#scarto le frasi troppo lunghe\r\n","condition = [len(x.split(\" \")) <= TEST_MAX_LEN  for x in test_dataset_frame[\"question\"]]\r\n","test_dataset_frame  = test_dataset_frame[condition]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOVFYn4cAsvM"},"source":["## Tokenization"]},{"cell_type":"code","metadata":{"id":"bkc6YUJetm0Q"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","question_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","question_tokenizer.fit_on_texts(dataset_frame[\"question\"])\n","dataset_frame[\"tokenQuestion\"] = question_tokenizer.texts_to_sequences(dataset_frame[\"question\"])\n","word_to_int = question_tokenizer.word_index\n","max_question_len = max(len(sentence) for sentence in dataset_frame[\"tokenQuestion\"])\n","#considero pure il padding per il numero di parole\n","num_words = len(word_to_int) + 1\n","print(\"Max question len after preprocessing: {}\\nNum of words: {}\".format(max_question_len, num_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIqDCrRq27EL"},"source":["num_tokens = num_words\r\n","hits = 0\r\n","misses = 0\r\n","\r\n","# Prepare embedding matrix\r\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n","for word, i in word_to_int.items():\r\n","    embedding_vector = embeddings_index.get(word)\r\n","    if embedding_vector is not None:\r\n","        # Words not found in embedding index will be all-zeros.\r\n","        # This includes the representation for \"padding\" and \"OOV\"\r\n","        embedding_matrix[i] = embedding_vector\r\n","        hits += 1\r\n","    else:\r\n","        misses += 1\r\n","print(\"Converted %d words (%d misses)\" % (hits, misses))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_8I4fThFbKu"},"source":["test_dataset_frame[\"tokenQuestion\"] = question_tokenizer.texts_to_sequences(test_dataset_frame[\"question\"])\r\n","test_max_question_len = max(len(sentence) for sentence in test_dataset_frame[\"tokenQuestion\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tnVrXP8iDs6"},"source":["## Generators\n"]},{"cell_type":"markdown","metadata":{"id":"4boGeRa1icOT"},"source":["We build two generators, the first for the training and validation data which yields __[input_sequence, input_image], labels__, the second for the test yielding just __[input_sequence, input_image]__."]},{"cell_type":"code","metadata":{"id":"HveD4kw2eWbO"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","from PIL import Image\r\n","\r\n","prepr_func = tf.keras.applications.vgg16.preprocess_input\r\n","\r\n","#È fondamentale non mettere nessun parametro shuffle = True, se si vuole mischiare, \r\n","#fare shuffle sul dataframe. data_gen_args è un dictionary che contiene valori \r\n","#per la data augmntation.\r\n","def custom_generator(dataframe, batch_size, target_h, target_w, data_gen_args=None):\r\n","  #list in which the padded questions are considered\r\n","  padded_questions = pad_questions(dataframe)\r\n","  #image generator loading\r\n","  image_generator = ImageDataGenerator()\r\n","\r\n","  curr_index = 0\r\n","  while(True):\r\n","    #estrazione delle prossime batch_size frasi\r\n","    input_sequences = []\r\n","    input_images = []\r\n","    labels = []\r\n","    i = curr_index\r\n","    for _ in range(batch_size):\r\n","      index = i % len(dataframe[\"index\"])\r\n","      #estrai frase\r\n","      input_sequences.append(extract_sequence(padded_questions, index))\r\n","      #estrai immagine\r\n","      input_images.append(extract_image(dataframe, index, target_h, target_w, image_generator)) \r\n","      #estrazione della label\r\n","      label = dataframe[\"answer\"][index]\r\n","      labels.append(label)\r\n","\r\n","      i+=1\r\n","\r\n","    curr_index += batch_size\r\n","\r\n","    #adding the batch dimension\r\n","    input_sequences = np.array(input_sequences)\r\n","    input_images = np.array(input_images)\r\n","    labels = np.array(labels)\r\n","    \r\n","    yield [input_sequences, input_images], labels\r\n","\r\n","def test_generator(dataframe, batch_size, target_h, target_w, data_gen_args=None):\r\n","  padded_questions = pad_questions(dataframe)\r\n","  #image generator loading\r\n","  image_generator = ImageDataGenerator()\r\n","  curr_index = 0\r\n","  while(True):\r\n","    #estrazione delle prossime batch_size frasi\r\n","    input_sequences = []\r\n","    input_images = []\r\n","    i = curr_index\r\n","    for _ in range(batch_size):\r\n","      index = i % len(dataframe[\"index\"])\r\n","      #estrai frase\r\n","      input_sequences.append(extract_sequence(padded_questions, index))\r\n","      #estrai immagine\r\n","      input_images.append(extract_image(dataframe, index, target_h, target_w, image_generator)) \r\n","      i+=1\r\n","\r\n","    curr_index += batch_size\r\n","\r\n","    #adding the batch dimension\r\n","    input_sequences = np.array(input_sequences)\r\n","    input_images = np.array(input_images)\r\n","\r\n","    yield [input_sequences, input_images]\r\n","\r\n","def pad_questions(dataframe):\r\n","  #list in which the padded questions are considered\r\n","  padded_questions = None\r\n","  #data padding\r\n","  max_question_len = max(len(sentence) for sentence in dataframe[\"tokenQuestion\"])\r\n","  padded_questions = pad_sequences(dataframe[\"tokenQuestion\"], maxlen=max_question_len, padding=\"post\")\r\n","  return padded_questions\r\n","\r\n","def extract_sequence(questions, index):\r\n","  return questions[index]\r\n","\r\n","def extract_image(dataframe, index, target_h, target_w, image_generator):\r\n","  image_id = dataframe[\"image_id\"][index]\r\n","  image_path = os.path.join(\"VQA_Dataset\", \"Images\", str(image_id)+\".png\")\r\n","  image = Image.open(image_path).resize((target_w, target_h)).convert('RGB')\r\n","  image_arr = np.array(image)\r\n","  if prepr_func != None:\r\n","    image_arr = prepr_func(image_arr)\r\n","  return image_arr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yo6Mx01rjAqr"},"source":["## Prepare for Training\n","This notebook allows to perform K-fold cross-validation by setting the respective parameter to True at the beginning. The model built uses VGG to implement the CNN to perform the feature extraction while a single LSTM block realizes the RNN. The features and output of the LSTM are finally concatenated into a dense layer. The model realized is compiled with __sparse_categorical_crossentropy__ and __Adam_optimizer__, before starting the fitting procedure."]},{"cell_type":"code","metadata":{"id":"1bi7JjWeeI72"},"source":["from datetime import datetime\n","\n","exps_dir = '/content/drive/My Drive/challenge3/'\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_dir = os.path.join(exps_dir, '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LR_FXO3rlrSJ"},"source":["import math\r\n","from keras.applications import VGG16\r\n","from sklearn.model_selection import KFold\r\n","\r\n","kfold = KFold(n_splits=k, random_state=SEED, shuffle=True)\r\n","\r\n","loop_iteration = 0\r\n","loss_arr = []\r\n","\r\n","for train_index, val_index in kfold.split(dataset_frame, dataset_frame[\"answer\"]):\r\n","  train_dataframe = dataset_frame.iloc[train_index]\r\n","  val_dataframe = dataset_frame.iloc[val_index]\r\n","  train_dataframe.reset_index(level=0, inplace=True)\r\n","  val_dataframe.reset_index(level=0, inplace=True)\r\n","\r\n","  train_generator = custom_generator(train_dataframe, bs, img_h, img_w)\r\n","  val_generator = custom_generator(train_dataframe, bs, img_h, img_w)\r\n","\r\n","  # CNN for iamges\r\n","  image_input = tf.keras.Input(shape=(img_h, img_w, 3))\r\n","\r\n","  vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\r\n","  vgg16.trainable = False\r\n","  vgg_out = vgg16(image_input)\r\n","\r\n","  x = tf.keras.layers.Flatten()(vgg_out)\r\n","  x = tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(1e-5), activation='relu')(x)\r\n","  x = tf.keras.layers.Dropout(0.3)(x)\r\n","\r\n","  # LSTM for text\r\n","  encoder_input = tf.keras.Input(shape=[max_question_len])\r\n","  encoder_embedding_out = tf.keras.layers.Embedding(\r\n","      num_words,\r\n","      embedding_dim,\r\n","      embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\r\n","      input_length=max_question_len,\r\n","      mask_zero=True\r\n","  )(encoder_input)\r\n","\r\n","  #Embedding(num_words, EMBEDDING_SIZE, input_length=max_question_len, mask_zero=True)(encoder_input)\r\n","\r\n","  #encoder_embedding_out = encoder_embedding_layer(encoder_input)\r\n","  encoder = tf.keras.layers.LSTM(units=512)(encoder_embedding_out)\r\n","\r\n","  # finally concatenate\r\n","  concatenated = tf.keras.layers.concatenate([x, encoder], axis=-1)\r\n","  output = tf.keras.layers.Dense(num_classes, activation='softmax')(concatenated)\r\n","  model = tf.keras.Model([encoder_input, image_input], output)\r\n","\r\n","  # Loss\r\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n","\r\n","  # learning rate\r\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","  # Validation metrics\r\n","  metrics = ['accuracy']\r\n","\r\n","  # Compile Model\r\n","  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n","\r\n","  callbacks = []\r\n","\r\n","  # Model checkpoint\r\n","  # ----------------\r\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","  if not os.path.exists(ckpt_dir):\r\n","      os.makedirs(ckpt_dir)\r\n","\r\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'),\r\n","                                                      monitor='val_loss',\r\n","                                                      mode='min', \r\n","                                                      save_weights_only=True,\r\n","                                                      save_best_only=True,\r\n","                                                      verbose=0)  # False to save the model directly\r\n","  callbacks.append(ckpt_callback)\r\n","\r\n","\r\n","  # Early Stopping\r\n","  # --------------\r\n","  early_stop = True\r\n","  if early_stop:\r\n","      es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\r\n","      callbacks.append(es_callback)\r\n","\r\n","  history = model.fit(x = train_generator,\r\n","          epochs=num_epochs,\r\n","          steps_per_epoch=math.ceil(len(train_dataframe)/bs),\r\n","          validation_data = val_generator,\r\n","          validation_steps=math.ceil(len(val_dataframe)/bs),\r\n","          callbacks=callbacks)\r\n","  \r\n","  minLoss = min(history.history['val_loss'])\r\n","  minLossIndex = history.history['val_loss'].index(minLoss)\r\n","  loss_arr.append(minLoss)\r\n","  \r\n","  # print metrics to file\r\n","  with open(os.path.join(exp_dir, 'historySplit' + str(loop_iteration) + '.txt'), 'w') as f:\r\n","    for key in history.history.keys():\r\n","      print(str(key), file=f)\r\n","      print(history.history[key], file=f)\r\n","\r\n","  if not enable_kfold:\r\n","    break\r\n","  \r\n","  loop_iteration += 1\r\n","\r\n","with open(os.path.join(exp_dir, 'cv_results' + '.txt'), 'w') as f2:\r\n","  print(\"avg loss = {}\".format(np.mean(loss_arr)), file=f2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cq0bWLDkDAE"},"source":["## Prepare for testing\n","Once the generator is istantiated we make the prediction and realize the csv used for the submission."]},{"cell_type":"code","metadata":{"id":"jh845b4lhoQ9"},"source":["test_gen = test_generator(test_dataset_frame, bs, img_h, img_w)\r\n","prediction = model.predict(test_gen, steps=math.ceil(len(test_dataset_frame)/bs))\r\n","predicted_class = np.argmax(prediction, axis=-1)\r\n","ciccia = math.ceil(len(test_dataset_frame)/bs) * bs - len(test_dataset_frame)\r\n","predicted_class = predicted_class[:-ciccia]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTdxWUSu0aco"},"source":["test_dict = {}\r\n","i = 0\r\n","for index in test_dataset_frame[\"index\"]:\r\n","  test_dict[index] = predicted_class[i]\r\n","  i+=1 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DgSIxk4VkUGx"},"source":["### Build the csv file"]},{"cell_type":"code","metadata":{"id":"Z4aR2eyk0bg7"},"source":["import os\r\n","from datetime import datetime\r\n","\r\n","def create_csv(results, results_dir='./'):\r\n","\r\n","    csv_fname = 'results_'\r\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\r\n","\r\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\r\n","\r\n","        f.write('Id,Category\\n')\r\n","\r\n","        for key, value in results.items():\r\n","            f.write(key + ',' + str(value) + '\\n')\r\n","\r\n","create_csv(test_dict, exp_dir)"],"execution_count":null,"outputs":[]}]}