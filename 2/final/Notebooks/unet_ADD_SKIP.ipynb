{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"unet_ADD_SKIP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sQ4ZIw8V5eCI"},"source":["# UNET_ADD_SKIP\n","The following notebook implements the best unet architecture we were able to achieve. Images are split in tiles for training, each with size 512x512. This architecture uses two kind of connections between the five blocks on whic is built:\n","* __ADD_Connections__: allow to concatenate \"vertically\" consecutive blocks by adding the MaxPooling layer of the considered block with the one of the previous\n","* __SKIP_Connections__: allow to concatenate \"in parallel\" blocks as described in the paper describing the unet architecture\n","\n","The model obtained is compiled with __Adam__ optimizer and two metrics: the __Accuracy__ and the __Intersection_Over_the_Union__. The loss used is a __weighted_categorical_cross_entropy__ which tries to give more importance to the classes that are less distinguishable in the images. To do so, respectively for __Background__, __Crop__ and __Weed__, we assigned the weights: 0.5, 1 and 1.5."]},{"cell_type":"markdown","metadata":{"id":"b6BmvvNL8H24"},"source":["## Notebook Settings"]},{"cell_type":"markdown","metadata":{"id":"8j5oGewFx6i-"},"source":["This notebook was used to generate the best models trained on the following datasets:\n","* __Bipbip Haricot__\n","* __Pead Haricot__\n","* __Weedelec Haricot__\n","\n","In order to select the correct dataset fill with a boolean variable the elements of the __bool_arr__ list in the next cell."]},{"cell_type":"code","metadata":{"id":"mQ3XnuTye_vO"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","SEED = 1234\n","img_h = 512\n","img_w = 512\n","bs = 8\n","lr = 1e-3\n","num_epochs = 100\n","patience = 15\n","num_classes = 3\n","debug_mode = False # true if you wanna use the reduced dataset, with only 3 imgs per folder\n","\n","# kfold\n","val_split_perc = 0.2\n","k = 3\n","enable_kfold = False\n","if not enable_kfold:\n","  k = int(1 / val_split_perc)\n","\n","# boolean flags\n","# choose here what to use for training\n","bool_arr = []\n","bool_arr.append([True, \"Bipbip\", \"Haricot\", \".jpg\"])\n","bool_arr.append([False, \"Bipbip\", \"Mais\", \".jpg\"])\n","bool_arr.append([False, \"Pead\", \"Haricot\", \".jpg\"])\n","bool_arr.append([False, \"Pead\", \"Mais\", \".jpg\"])\n","bool_arr.append([False, \"Roseau\", \"Haricot\", \".jpg\"])\n","bool_arr.append([False, \"Roseau\", \"Mais\", \".jpg\"])\n","bool_arr.append([False, \"Weedelec\", \"Haricot\", \".jpg\"])\n","bool_arr.append([False, \"Weedelec\", \"Mais\", \".jpg\"])\n","\n","# choose here what to use for testing\n","bool_arr_test = []\n","bool_arr_test.append([True, \"Bipbip\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Bipbip\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Roseau\", \"Haricot\", \".png\"])\n","bool_arr_test.append([True, \"Roseau\", \"Mais\", \".png\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Mais\", \".jpg\"])\n","\n","model_name = 'final_unet_SKIP_ADD'\n","\n","tf.random.set_seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNv8g2ne8Nl5"},"source":["### Mount Google Drive and Unzip the Dataset"]},{"cell_type":"code","metadata":{"id":"WacDhwdYfA77"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SdCO9508TC7"},"source":["For debugging purposes we first used a reduced dataset and then moved to the one containing the tiles of all the images called __Development_Dataset_512__"]},{"cell_type":"code","metadata":{"id":"x43-ArnVfRHA"},"source":["cwd = os.getcwd() # should be /content\n","dataset_version = 'Reduced_Development_Dataset' if debug_mode else 'Final_Dataset_512'\n","\n","if debug_mode:\n","  if not os.path.exists(os.path.join(cwd, dataset_version)):\n","    !unzip '/content/drive/My Drive/challenge2/dataset/Reduced_Development_Dataset.zip'\n","else:\n","  if not os.path.exists(os.path.join(cwd, dataset_version)):\n","    !unzip '/content/drive/My Drive/challenge2/dataset/Final_Dataset_512.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XX1W4fI8g5I"},"source":["## Data Preparation\n","Once we specify in the first cell of the __Notebook_Settings__ paragraph the images we want to train our network with, we use the pandas dataframes for managing them."]},{"cell_type":"code","metadata":{"id":"EcsG6HOa6-Fe"},"source":["import pandas as pd\r\n","\r\n","filenames_images = []\r\n","filenames_masks = []\r\n","\r\n","base_folder = os.path.join(cwd, dataset_version, \"Training\")\r\n","\r\n","for i in range(0,8):\r\n","  if bool_arr[i][0]:\r\n","    bf = []\r\n","    base_curr = os.path.join(base_folder, bool_arr[i][1], bool_arr[i][2])\r\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\r\n","    fn_images.sort()\r\n","    fn_masks = [x for x in os.listdir(os.path.join(base_curr, \"Masks\"))]\r\n","    fn_masks.sort()\r\n","\r\n","    for index, value in enumerate(fn_images):\r\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\r\n","\r\n","    for index, value in enumerate(fn_masks):\r\n","      fn_masks[index] = os.path.join(base_curr, \"Masks\", value)\r\n","\r\n","    filenames_images += fn_images\r\n","    filenames_masks += fn_masks\r\n","\r\n","base_folder = os.path.join(cwd, 'Final_Dataset_512', \"Training\")\r\n","\r\n","for i in range(0,8):\r\n","  if bool_arr[i][0]:\r\n","    bf = []\r\n","    base_curr = os.path.join(base_folder, bool_arr[i][1], bool_arr[i][2])\r\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\r\n","    fn_images.sort()\r\n","    fn_masks = [x for x in os.listdir(os.path.join(base_curr, \"Masks\"))]\r\n","    fn_masks.sort()\r\n","\r\n","    for index, value in enumerate(fn_images):\r\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\r\n","\r\n","    for index, value in enumerate(fn_masks):\r\n","      fn_masks[index] = os.path.join(base_curr, \"Masks\", value)\r\n","\r\n","    filenames_images += fn_images\r\n","    filenames_masks += fn_masks\r\n","\r\n","data = pd.DataFrame(columns=[\"images\", \"masks\"])\r\n","data[\"images\"] = filenames_images\r\n","data[\"masks\"] = filenames_masks\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBOY_v9t_bOU"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"id":"9IVnZ-QyiWWE"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","apply_data_augmentation = True\n","\n","# Create training ImageDataGenerator object\n","# We need two different generators for images and corresponding masks\n","if apply_data_augmentation:\n","    train_img_data_gen = ImageDataGenerator(rotation_range=30,\n","                                            width_shift_range=10,\n","                                            height_shift_range=10,\n","                                            zoom_range=0.3,\n","                                            horizontal_flip=True,\n","                                            vertical_flip=True,\n","                                            fill_mode='reflect',                                          \n","                                            rescale=1./255)\n","    \n","    train_mask_data_gen = ImageDataGenerator(rotation_range=30,\n","                                             width_shift_range=10,\n","                                             height_shift_range=10,\n","                                             zoom_range=0.3,\n","                                             horizontal_flip=True,\n","                                             vertical_flip=True,\n","                                             fill_mode='reflect')\n","else:\n","    train_img_data_gen = ImageDataGenerator(rescale=1./255)\n","    train_mask_data_gen = ImageDataGenerator()\n","\n","# Create validation and test ImageDataGenerator objects\n","valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n","valid_mask_data_gen = ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pd1AlzqT_sjM"},"source":["## Unet Model\n","The following cell defines the method which builds the Unet architecture with add and skip connections as defined at the beginning. By passing the number of classes on which the problem is defined we are able to parametrize the model also for more classes."]},{"cell_type":"code","metadata":{"id":"OxKuYsfbWb6r"},"source":["from tensorflow.keras.layers import *\r\n","\r\n","def upsampleLayer(in_layer, concat_layer, input_size):\r\n","  '''\r\n","  Upsampling (=Decoder) layer building block\r\n","  Parameters\r\n","  ----------\r\n","  in_layer: input layer\r\n","  concat_layer: layer with which to concatenate\r\n","  input_size: input size fot convolution\r\n","  '''\r\n","  upsample = Conv2DTranspose(input_size, (2, 2), strides=(2, 2), padding='same')(in_layer) \r\n","  \r\n","  upsample = concatenate([concat_layer, upsample])\r\n","  \r\n","  conv = Conv2D(input_size, (3, 3), activation='relu', padding='same')(upsample)\r\n","  conv = BatchNormalization()(conv)\r\n","  conv = Dropout(0.2)(conv)\r\n","\r\n","  return conv\r\n","\r\n","def create_model(num_classes):\r\n","\r\n","  inputs_1 = tf.keras.Input((img_h, img_w, 3))\r\n","\r\n","  # encoder\r\n","  e1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_1)\r\n","  e1b = MaxPool2D(pool_size=(2, 2))(e1)\r\n","\r\n","  e3 = Conv2D(64, (3, 3), activation='relu', padding='same')(e1b)\r\n","  e3b = MaxPool2D(pool_size=(2, 2))(e3)\r\n","  r3 = Conv2D(64, 1, strides=2, padding=\"same\")(e1b)\r\n","  e3b = add([e3b, r3])\r\n","\r\n","  e4 = Conv2D(128, (3, 3), activation='relu', padding='same')(e3b)\r\n","  e4b = MaxPool2D(pool_size=(2, 2))(e4)\r\n","  r4 = Conv2D(128, 1, strides=2, padding=\"same\")(e3b)\r\n","  e4b = add([e4b, r4])\r\n","\r\n","  e5 = Conv2D(256, (3, 3), activation='relu', padding='same')(e4b)\r\n","  e5b = MaxPool2D(pool_size=(2, 2))(e5)\r\n","  r5 = Conv2D(256, 1, strides=2, padding=\"same\")(e4b)\r\n","  e5b = add([e5b, r5])\r\n","\r\n","  # bottleneck\r\n","  e6 = Conv2D(512, (3, 3), activation='relu', padding='same')(e5b)\r\n","  e6b = MaxPool2D(pool_size=(2, 2))(e6)\r\n","  r6 = Conv2D(512, 1, strides=2, padding=\"same\")(e5b)\r\n","  e6b = add([e6b, r6])\r\n","\r\n","  # decoder\r\n","  d1 = upsampleLayer(in_layer=e6b, concat_layer=e5b, input_size=256)\r\n","  rd1 = UpSampling2D(2)(e6b)\r\n","  rd1 = Conv2D(256, 1, padding=\"same\")(rd1)\r\n","  d1 = add([d1, rd1])\r\n","\r\n","  d2 = upsampleLayer(in_layer=d1, concat_layer=e4b, input_size=128)\r\n","  rd2 = UpSampling2D(2)(d1)\r\n","  rd2 = Conv2D(128, 1, padding=\"same\")(rd2)\r\n","  d2 = add([d2, rd2])\r\n","\r\n","  d3 = upsampleLayer(in_layer=d2, concat_layer=e3b, input_size=64)\r\n","  rd3 = UpSampling2D(2)(d2)\r\n","  rd3 = Conv2D(64, 1, padding=\"same\")(rd3)\r\n","  d3 = add([d3, rd3])\r\n","\r\n","  d4 = upsampleLayer(in_layer=d3, concat_layer=e1b, input_size=32)\r\n","  rd4 = UpSampling2D(2)(d3)\r\n","  rd4 = Conv2D(32, 1, padding=\"same\")(rd4)\r\n","  d4 = add([d4, rd4])\r\n","\r\n","  d5 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(d4)\r\n","  d5 = Conv2D(16, (3, 3), activation='relu', padding='same')(d5)\r\n","  d5 = BatchNormalization()(d5)\r\n","  d5 = Dropout(0.2)(d5)\r\n","  rd5 = UpSampling2D(2)(d4)\r\n","  rd5 = Conv2D(16, 1, padding=\"same\")(rd5)\r\n","  d5 = add([d5, rd5])\r\n","\r\n","  outputs = Conv2D(num_classes, (1, 1), activation='softmax')(d5)\r\n","\r\n","  model = tf.keras.Model(inputs=inputs_1, outputs=outputs)\r\n","  \r\n","  return model\r\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWUZAov8BXDv"},"source":["## Compiling Methods\n","For the multiclass segmentation problem we are tackling we need to specify the __IoU__ metric and a specific loss function in order to balance the importance of the different classes."]},{"cell_type":"markdown","metadata":{"id":"kqWvDFpcBvpE"},"source":["### Intersection Over Union"]},{"cell_type":"code","metadata":{"id":"mPfgobNhW9OE"},"source":["def meanIoU(y_true, y_pred):\r\n","    # get predicted class from softmax\r\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n","\r\n","    per_class_iou = []\r\n","\r\n","    for i in range(1,num_classes): # exclude the background class 0\r\n","      # Get prediction and target related to only a single class (i)\r\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n","      intersection = tf.reduce_sum(class_true * class_pred)\r\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n","    \r\n","      iou = (intersection + 1e-7) / (union + 1e-7)\r\n","      per_class_iou.append(iou)\r\n","\r\n","    return tf.reduce_mean(per_class_iou)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHMbxZHnB7bd"},"source":["### Weighted Categorical Crossentropy Loss Function"]},{"cell_type":"code","metadata":{"id":"SeIz32Li8U2D"},"source":["from keras import backend as K\n","def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","\n","    def loss(y_true, y_pred):\n","      #print(\"y_true shape={}\\ny_pred shape={}\\n{}\".format(y_true.shape, y_pred.shape, y_true[:,:,:]))  \n","      # scale predictions so that the class probas of each sample sum to 1\n","      y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","      # clip to prevent NaN's and Inf's\n","      y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","      #trasformo da sparse representation a onehot representation per calcolare la loss\n","      #one_hot_encoding = np.zeros((y_true.shape[0], y_true.shape[1], y_true.shape[2], y_pred.shape[3]))\n","      #one_hot_encoding = K.zeros((y_true.shape[0], y_true.shape[1], y_true.shape[2], y_pred.shape[3]))\n","      one_hot_encoding = (tf.cast(tf.reduce_any(y_true == 0, axis=-1, keepdims=True), tf.float32)*tf.constant([1.,0.,0.]) +\n","                          tf.cast(tf.reduce_any(y_true == 1, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,1.,0.])+\n","                          tf.cast(tf.reduce_any(y_true == 2, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,0.,1.]))\n","                    \n","\n","      # calc\n","      # se non fosse one hot avrei tipo [[0,1,0],[0,0,1]] * [[a,b,c], [d,e,f]] = [[a,b,c], [2d,2e,2f]], ma io voglio [[0,b,0], [0,0,f]]\n","      #1 2 \n","      loss = one_hot_encoding * K.log(y_pred) * weights\n","      loss = -K.sum(loss, -1)\n","      return loss\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmSRbBViCC6L"},"source":["## Prepare for Training"]},{"cell_type":"markdown","metadata":{"id":"3DevzSkTCFhs"},"source":["### Callbacks\n","The model is trained with 3 different callbacks that help both in storing the checkpoints containing the best weights found and in facing overfitting. The callbacks used are:\n","* __Checkpoint_Callback__\n","* __Learning_Rate_Scheduler_Callback__\n","* __Early_Stopping_Callback__"]},{"cell_type":"code","metadata":{"id":"zGisKIPc93kf"},"source":["def get_callbacks(exp_dir):\r\n","  callbacks = []\r\n","\r\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","  if not os.path.exists(ckpt_dir):\r\n","      os.makedirs(ckpt_dir)\r\n","\r\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'),\r\n","                                                    monitor='val_loss',\r\n","                                                    mode='min', \r\n","                                                    save_weights_only=False,\r\n","                                                    save_best_only=True,\r\n","                                                    verbose=0)  \r\n","  callbacks.append(ckpt_callback)\r\n","\r\n","  # learning rate scheduler callback\r\n","  def scheduler(epoch, lr):\r\n","    if epoch < 10:\r\n","      return lr\r\n","    else:\r\n","      return lr * tf.math.exp(-0.1)\r\n","\r\n","  lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\n","\r\n","  lr_scheduler = True\r\n","  if lr_scheduler:\r\n","    lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\n","    callbacks.append(lr_callback)\r\n","\r\n","  # Early Stopping\r\n","  # --------------\r\n","  early_stop = True\r\n","  if early_stop:\r\n","      es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \r\n","                                                    patience=patience, \r\n","                                                    restore_best_weights=True)\r\n","      callbacks.append(es_callback)\r\n","\r\n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Yui8Zu1CxKp"},"source":["### Save all the Hyperparameters\n","For hyperparameters tuning for each of the model tested we write a file containing all its hyperparameters."]},{"cell_type":"code","metadata":{"id":"8dFfT5FJXyRh"},"source":["from datetime import datetime\r\n","\r\n","# put here the path where you want to save the checkpoints of your model\r\n","exps_dir = '/content/drive/MyDrive/challenge2/2_phase/final_experiments'\r\n","\r\n","if not os.path.exists(exps_dir):\r\n","    os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","    os.makedirs(exp_dir)\r\n","\r\n","file1 = open(os.path.join(exp_dir, \"parameters.txt\"),\"w\")\r\n","\r\n","file1.write('SEED = ' + str(SEED))\r\n","file1.write('\\nimg_w = ' + str(img_w))\r\n","file1.write('\\nimg_h = ' + str(img_h))\r\n","file1.write('\\nbs = ' + str(bs))\r\n","file1.write('\\nlr = ' + str(lr))\r\n","file1.write('\\npatience = ' + str(patience))\r\n","file1.write('\\nnum_epochs = ' + str(num_epochs))\r\n","file1.write('\\nnum_classes = ' + str(num_classes))\r\n","file1.write('\\ndebug_mode = ' + str(debug_mode))\r\n","file1.write('\\nenable_kfold = ' + str(enable_kfold))\r\n","if enable_kfold:\r\n","  file1.write('\\nk = ' + str(k))\r\n","else:\r\n","  file1.write('\\nval_split_perc = ' + str(val_split_perc))\r\n","\r\n","file1.write('\\nBipbip Haricot = ' + str(bool_arr[0][0]))\r\n","file1.write('\\nBipbip Mais = ' + str(bool_arr[1][0]))\r\n","file1.write('\\nPead Haricot = ' + str(bool_arr[2][0]))\r\n","file1.write('\\nPead Mais = ' + str(bool_arr[3][0]))\r\n","file1.write('\\nRoseau Haricot = ' + str(bool_arr[4][0]))\r\n","file1.write('\\nRoseau Mais = ' + str(bool_arr[5][0]))\r\n","file1.write('\\nWeedelec Haricot = ' + str(bool_arr[6][0]))\r\n","file1.write('\\nWeedelec Mais = ' + str(bool_arr[7][0]))\r\n","\r\n","file1.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIksDjVWDH3j"},"source":["### Start Training\n","The prepare function is used for mapping the colors of the masks to the respective classes. Remember that we have:\n","* RGB: 0 0 0 - Target 0 (background)\n","* RGB: 254 124 18 - Target 0 (background)\n","* RGB: 255 255 255 - Target 1 (crop)\n","* RGB: 216 67 82 - Target 2 (weed)"]},{"cell_type":"code","metadata":{"id":"CN13t7Vw_o7Q"},"source":["def prepare_target(x_, y_):\n","    return x_, (tf.cast(tf.reduce_any(y_ == 0, axis=-1, keepdims=True), tf.float32)*0 + \n","                tf.cast(tf.reduce_any(y_ == 124, axis=-1, keepdims=True), tf.float32)*0 + \n","                tf.cast(tf.reduce_any(y_ == 255, axis=-1, keepdims=True), tf.float32)*1 + \n","                tf.cast(tf.reduce_any(y_ == 67, axis=-1, keepdims=True), tf.float32)*2) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnNp_SpnDlsD"},"source":["The training part allows for k-fold. In the case we are making cross validation we need first to specify the number of datasets k that we want to consider and then execute this cell. At the end we will have the best model among all the ones tested on the different datasets."]},{"cell_type":"code","metadata":{"id":"L5eNdD5J-pVn"},"source":["from sklearn.model_selection import KFold\r\n","\r\n","kfold = KFold(n_splits=k, random_state=SEED, shuffle=True)\r\n","\r\n","loop_iteration = 0\r\n","\r\n","loss_arr = []\r\n","meanIoU_arr = []\r\n","\r\n","for train_index, val_index in kfold.split(X=data[\"images\"], y=data[\"masks\"]):\r\n","  training_data = data.iloc[train_index]\r\n","  validation_data = data.iloc[val_index]\r\n","\r\n","  #creation of the couple of generator for images and masks from training\r\n","  train_img_data_generator = train_img_data_gen.flow_from_dataframe(training_data,\r\n","                                                                    x_col = \"images\",\r\n","                                                                    shuffle = True,\r\n","                                                                    class_mode = None,\r\n","                                                                    target_size=(img_h, img_w),\r\n","                                                                    batch_size=bs,\r\n","                                                                    interpolation=\"nearest\",\r\n","                                                                    seed=SEED)\r\n","  train_mask_data_generator = train_mask_data_gen.flow_from_dataframe(training_data,\r\n","                                                                      x_col = \"masks\",\r\n","                                                                      shuffle = True,\r\n","                                                                      class_mode = None,\r\n","                                                                      target_size=(img_h, img_w),\r\n","                                                                      batch_size=bs,\r\n","                                                                      interpolation=\"nearest\",\r\n","                                                                      seed=SEED)\r\n","  train_gen = zip(train_img_data_generator, train_mask_data_generator)\r\n","\r\n","  #creation of the couple of generators for images and masks for validation  \r\n","  valid_img_data_generator = valid_img_data_gen.flow_from_dataframe(validation_data,\r\n","                                                       x_col = \"images\",\r\n","                                                       shuffle = True,\r\n","                                                       class_mode = None,\r\n","                                                       target_size=(img_h, img_w),\r\n","                                                       batch_size=bs,\r\n","                                                       interpolation=\"nearest\",\r\n","                                                       seed = SEED)\r\n","  valid_mask_data_generator = valid_mask_data_gen.flow_from_dataframe(validation_data, \r\n","                                                      x_col = \"masks\",\r\n","                                                      shuffle = True,\r\n","                                                      class_mode = None,\r\n","                                                      target_size=(img_h, img_w),\r\n","                                                      batch_size=bs,\r\n","                                                      interpolation=\"nearest\",\r\n","                                                      seed = SEED)\r\n","  valid_gen = zip(valid_img_data_generator, valid_mask_data_generator)\r\n","\r\n","  #######################\r\n","\r\n","  train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\r\n","  train_dataset = train_dataset.map(prepare_target)\r\n","  train_dataset = train_dataset.repeat()\r\n","\r\n","  valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \r\n","                                                output_types=(tf.float32, tf.float32),\r\n","                                                output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\r\n","  valid_dataset = valid_dataset.map(prepare_target)\r\n","  valid_dataset = valid_dataset.repeat()\r\n","\r\n","  #####################################################################################################################\r\n","\r\n","  model = create_model(num_classes)\r\n","  model.summary()\r\n","\r\n","  loss = weighted_categorical_crossentropy(np.asarray([0.5,1.,1.5], dtype=np.float32))\r\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","  metrics = ['accuracy', meanIoU]\r\n","\r\n","  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n","\r\n","  callbacks = get_callbacks(exp_dir)\r\n","\r\n","  history = model.fit(x=train_dataset,\r\n","              epochs=num_epochs,\r\n","              steps_per_epoch=len(train_img_data_generator),\r\n","              validation_data=valid_dataset,\r\n","              validation_steps=len(valid_img_data_generator), \r\n","              callbacks=callbacks)\r\n","\r\n","  minLoss = min(history.history['val_loss'])\r\n","  minLossIndex = history.history['val_loss'].index(minLoss)\r\n","  loss_arr.append(minLoss)\r\n","  meanIoU_arr.append(history.history['val_meanIoU'][minLossIndex])\r\n","  \r\n","  # print metrics to file\r\n","  with open(os.path.join(exp_dir, 'historySplit' + str(loop_iteration) + '.txt'), 'w') as f:\r\n","    for key in history.history.keys():\r\n","      print(str(key), file=f)\r\n","      print(history.history[key], file=f)\r\n","  \r\n","  if best_model_so_far == None:\r\n","    best_model_so_far = model\r\n","\r\n","  if not enable_kfold:\r\n","    break\r\n","  \r\n","  loop_iteration += 1\r\n","\r\n","with open(os.path.join(exp_dir, 'cv_results' + '.txt'), 'w') as f2:\r\n","  print(\"avg loss = {}\".format(np.mean(loss_arr)), file=f2)\r\n","  print(\"avg meanIoU = {}\".format(np.mean(meanIoU_arr)), file=f2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IolOEzVSEByd"},"source":["## Prepare for Testing"]},{"cell_type":"markdown","metadata":{"id":"T60xkO_LEQdk"},"source":["### Find the Filenames\n","The following cell builds a list containing the filenames of all the images that we want to test on. In order to specify the team and plant we choose, use the __bool_arr_test__ list specified in the first cell of the notebook."]},{"cell_type":"code","metadata":{"id":"cxBU6H5NDWkL"},"source":["test_images = []\r\n","\r\n","base_folder = os.path.join(cwd, dataset_version, \"Test_Final\")\r\n","\r\n","for i in range(0,8):\r\n","  if bool_arr_test[i][0]:\r\n","    team = []\r\n","    crop = []\r\n","    names = []\r\n","    base_curr = os.path.join(base_folder, bool_arr_test[i][1], bool_arr_test[i][2])\r\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\r\n","    fn_images.sort()\r\n","    for entry in fn_images:\r\n","      names.append(entry[:-4])\r\n","\r\n","    for j in range(0, len(fn_images)):\r\n","        team.append(bool_arr_test[i][1])\r\n","        crop.append(bool_arr_test[i][2])\r\n","    for index, value in enumerate(fn_images):\r\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\r\n","\r\n","    zipped_list = list(zip(fn_images, team, crop, names))\r\n","\r\n","    test_images += zipped_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SDjkm2gE58W"},"source":["### Testing Utils\n","The __rle_encode__ method is taken from the starting kit and is used to encode the prediction done on each image."]},{"cell_type":"code","metadata":{"id":"v6NuMFELiXkj"},"source":["def rle_encode(img):\r\n","    '''\r\n","    img: numpy array, 1 - foreground, 0 - background\r\n","    Returns run length as string formatted\r\n","    '''\r\n","    pixels = img.flatten()\r\n","    pixels = np.concatenate([[0], pixels, [0]])\r\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n","    runs[1::2] -= runs[::2]\r\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3iDWAGdXFFz2"},"source":["The following methods are used for tiling the test images once they have been loaded in memory. We need two functions to make the prediction in the correct way:\n","* __get_patches__: returns the tiles of an image in the specified size\n","* __reconstruct_from_patches__: builds back the image once we provide a stack of tiles"]},{"cell_type":"code","metadata":{"id":"QItAJTJ_lbEA"},"source":["# tiling utils\n","tile_size = 512\n","\n","def get_patches(img_arr, size=256, stride=256):\n","\n","    patches_list = []\n","    i_max = img_arr.shape[0] // stride\n","    j_max = img_arr.shape[1] // stride\n","\n","    for i in range(i_max):\n","        for j in range(j_max):\n","            patches_list.append(\n","                img_arr[\n","                    i * stride : i * stride + size,\n","                    j * stride : j * stride + size\n","                ]\n","            )\n","\n","    return np.stack(patches_list)\n","\n","def reconstruct_from_patches(img_arr, org_img_size, stride, size):\n","\n","    if img_arr.ndim == 3:\n","        img_arr = np.expand_dims(img_arr, axis=0)\n","\n","    if size is None:\n","        size = img_arr.shape[1]\n","\n","    if stride is None:\n","        stride = size\n","\n","    nm_layers = img_arr.shape[3]\n","\n","    i_max = (org_img_size[0] // stride) + 1 - (size // stride)\n","    j_max = (org_img_size[1] // stride) + 1 - (size // stride)\n","\n","    total_nm_images = img_arr.shape[0] // (j_max * i_max)\n","    nm_images = img_arr.shape[0]\n","\n","    averaging_value = size // stride\n","    images_list = []\n","    kk = 0\n","    for img_count in range(total_nm_images):\n","        img_bg = np.zeros(\n","            (org_img_size[0], org_img_size[1], nm_layers), dtype=img_arr[0].dtype\n","        )\n","\n","        for i in range(i_max):\n","            for j in range(j_max):\n","                for layer in range(nm_layers):\n","                    img_bg[\n","                        i * stride : i * stride + size,\n","                        j * stride : j * stride + size,\n","                        layer,\n","                    ] = img_arr[kk, :, :, layer]\n","\n","                kk += 1\n","\n","        images_list.append(img_bg)\n","    return np.stack(images_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiRZQg-6FpFB"},"source":["### Start Testing\n","Tiling is applied on test images at runtime. Each image is read with the respective filename, resized to the nearest multiple of the chosen tile size and cropped. Once we have the tiles we make the prediction on each of them, build a stack with the predictions and then reconstruct the full prediction with the same dimensions of the original image. At this point we can call the __rle_encode__ and insert in the submission dictionary the results for that image."]},{"cell_type":"code","metadata":{"id":"VUYXIKl5lc00"},"source":["import json\n","from PIL import Image\n","\n","res_dir = '/content/drive/My Drive/challenge2/2_phase/Results'\n","\n","# json generation\n","# ---------------\n","submission_dict = {}\n","\n","for entry in test_images:\n","  \n","  image = Image.open(entry[0])\n","  width, height = image.size\n","\n","  # resize image and create crops\n","  image = image.resize(((width // tile_size)*tile_size, (height // tile_size)*tile_size))\n","  img_arr = np.array(image)\n","  image_crops = get_patches(img_arr, size=tile_size, stride=tile_size)\n","\n","  # prediction on each tile stacking each result\n","  tile_mask_list = []\n","  for i in range(len(image_crops)):\n","    tile_arr = image_crops[i]\n","    tile_arr = tile_arr * 1. / 255\n","    \n","    out_sigmoid = model.predict(x=tf.expand_dims(tile_arr, 0))\n","    \n","    predicted_class = tf.argmax(out_sigmoid, -1)\n","    predicted_class = predicted_class[0, ...]\n","\n","    tile_mask_list.append(np.array(tf.expand_dims(predicted_class, axis=-1)))\n","  \n","  mask_crops = np.stack(tile_mask_list)\n","\n","  # reconstruct and resize\n","  mask_reconstructed = reconstruct_from_patches(mask_crops, org_img_size=(image.height, image.width), stride=tile_size, size=tile_size)\n","  \n","  disegno = np.zeros((image.height, image.width , 3))\n","  disegno[np.where(mask_reconstructed[0,...,0] == 1)] = [255, 255, 255]\n","  disegno[np.where(mask_reconstructed[0,...,0] == 0)] = [0,0,0]\n","  disegno[np.where(mask_reconstructed[0,...,0] == 2)] = [216, 67, 82]\n","\n","  imm = Image.fromarray(np.uint8(disegno)).resize((width, height))\n","  mask_arr = np.array(imm)\n","\n","  new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n","\n","  new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n","  new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n","\n","  img_name = entry[3]\n","\n","  submission_dict[img_name] = {}\n","  submission_dict[img_name]['shape'] = new_mask_arr.shape\n","  submission_dict[img_name]['team'] = entry[1]\n","  submission_dict[img_name]['crop'] = entry[2]\n","  submission_dict[img_name]['segmentation'] = {}\n","\n","  # RLE encoding\n","  # crop\n","  rle_encoded_crop = rle_encode(new_mask_arr == 1)\n","  # weed\n","  rle_encoded_weed = rle_encode(new_mask_arr == 2)\n","\n","  submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","  submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","\n","# Finally, save the results into the submission.json file\n","with open(os.path.join(res_dir, \"submission.json\"), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}