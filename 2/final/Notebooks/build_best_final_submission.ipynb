{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"build_best_final_submission.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7F_A2a-N2VWT"},"source":["# Best Submission Builder\n","This notebook allows to load a different model for each team and plant in order to make a more precise prediction on each of the datasets. Hence, it is important to have the checkpoint of each model, load them and prepare the filenames for the predict. In the end the submission dictionary is written in the json used for the submission"]},{"cell_type":"markdown","metadata":{"id":"LNdOUsKc3Gu3"},"source":["## Notebook settings"]},{"cell_type":"code","metadata":{"id":"7TsFb-zewaGY"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import json\n","from PIL import Image\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqzI-kuIwlE4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-XLETAkwluQ"},"source":["cwd = os.getcwd() # should be /content\n","dataset_version = 'Final_Filtered_Dataset_512'\n","dataset_version_noTile = 'Final_Dataset'\n","\n","if not os.path.exists(os.path.join(cwd, dataset_version)):\n","  !unzip '/content/drive/My Drive/challenge2/dataset/Final_Filtered_Dataset_512.zip'\n","\n","if not os.path.exists(os.path.join(cwd, dataset_version_noTile)):\n","  !unzip '/content/drive/My Drive/challenge2/dataset/Final_Dataset.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_V581TIm3m85"},"source":["## Prepare for loading\n","In order to load back the models we need to specify all the custom object used at compile time and pass them to the __load_model__ method provided by Keras."]},{"cell_type":"markdown","metadata":{"id":"HqNIo5wH310x"},"source":["### Intersection Over Union"]},{"cell_type":"code","metadata":{"id":"c5m1bmKlwxeg"},"source":["def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,num_classes): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQGHYTqW3_B6"},"source":["def cropIoU(y_true, y_pred):\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","    i = 1\n","    class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","    class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","    intersection = tf.reduce_sum(class_true * class_pred)\n","    union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    iou = (intersection + 1e-7) / (union + 1e-7)\n","    return iou\n","\n","def weedIoU(y_true, y_pred):\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","    i = 2\n","    class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","    class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","    intersection = tf.reduce_sum(class_true * class_pred)\n","    union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    iou = (intersection + 1e-7) / (union + 1e-7)\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inU96fSP34zS"},"source":["### Weighted Categorical Crossentropy"]},{"cell_type":"code","metadata":{"id":"UFNMnaNmw2ja"},"source":["from keras import backend as K\n","def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","\n","\n","\n","    def loss(y_true, y_pred):\n","      #print(\"y_true shape={}\\ny_pred shape={}\\n{}\".format(y_true.shape, y_pred.shape, y_true[:,:,:]))  \n","      # scale predictions so that the class probas of each sample sum to 1\n","      y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","      # clip to prevent NaN's and Inf's\n","      y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","      #trasformo da sparse representation a onehot representation per calcolare la loss\n","      #one_hot_encoding = np.zeros((y_true.shape[0], y_true.shape[1], y_true.shape[2], y_pred.shape[3]))\n","      #one_hot_encoding = K.zeros((y_true.shape[0], y_true.shape[1], y_true.shape[2], y_pred.shape[3]))\n","      one_hot_encoding = (tf.cast(tf.reduce_any(y_true == 0, axis=-1, keepdims=True), tf.float32)*tf.constant([1.,0.,0.]) +\n","                          tf.cast(tf.reduce_any(y_true == 1, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,1.,0.])+\n","                          tf.cast(tf.reduce_any(y_true == 2, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,0.,1.]))\n","                    \n","\n","      # calc\n","      #se  non fosse one hot avrei tipo [[0,1,0],[0,0,1]] * [[a,b,c], [d,e,f]] = [[a,b,c], [2d,2e,2f]], ma io voglio [[0,b,0], [0,0,f]]\n","      #1 2 \n","      loss = one_hot_encoding * K.log(y_pred) * weights\n","      loss = -K.sum(loss, -1)\n","      return loss\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgBEEjK74B59"},"source":["## Prepare for Testing\n","Since different models should be loaed we have to prepare all the methods needed to make the predict. Hence both the scripts for testing a model on the tiles and the ones for testing on the full images."]},{"cell_type":"code","metadata":{"id":"aOR8VMmWw3Qp"},"source":["def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDvX9Uesw-zL"},"source":["# tiling utils\n","tile_size = 512\n","\n","def get_patches(img_arr, size=256, stride=256):\n","\n","    patches_list = []\n","    i_max = img_arr.shape[0] // stride\n","    j_max = img_arr.shape[1] // stride\n","\n","    for i in range(i_max):\n","        for j in range(j_max):\n","            patches_list.append(\n","                img_arr[\n","                    i * stride : i * stride + size,\n","                    j * stride : j * stride + size\n","                ]\n","            )\n","\n","    return np.stack(patches_list)\n","\n","def reconstruct_from_patches(img_arr, org_img_size, stride, size):\n","\n","    if img_arr.ndim == 3:\n","        img_arr = np.expand_dims(img_arr, axis=0)\n","\n","    if size is None:\n","        size = img_arr.shape[1]\n","\n","    if stride is None:\n","        stride = size\n","\n","    nm_layers = img_arr.shape[3]\n","\n","    i_max = (org_img_size[0] // stride) + 1 - (size // stride)\n","    j_max = (org_img_size[1] // stride) + 1 - (size // stride)\n","\n","    total_nm_images = img_arr.shape[0] // (j_max * i_max)\n","    nm_images = img_arr.shape[0]\n","\n","    averaging_value = size // stride\n","    images_list = []\n","    kk = 0\n","    for img_count in range(total_nm_images):\n","        img_bg = np.zeros(\n","            (org_img_size[0], org_img_size[1], nm_layers), dtype=img_arr[0].dtype\n","        )\n","\n","        for i in range(i_max):\n","            for j in range(j_max):\n","                for layer in range(nm_layers):\n","                    img_bg[\n","                        i * stride : i * stride + size,\n","                        j * stride : j * stride + size,\n","                        layer,\n","                    ] = img_arr[kk, :, :, layer]\n","\n","                kk += 1\n","\n","        images_list.append(img_bg)\n","    return np.stack(images_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm6ScIcF4ev3"},"source":["### Load the Models\n","The following cell allows to load the models once we provide the path containing the __saved_model.pb__ file of the respective model. Remember that for the specific model you are loading you need to specify in the __custom_objects__ dictionary all the custom methods that were used at compile time. To split the prediction of each model on the respective dataset we build two lists in this order: __bh - bm - ph - pm - rh - rm - wh - wm__. The lists contain:\n","* __models_list__: list of all the loaded models\n","* __models_images_filenames__: list of lists containing the filenames of all the images on which the corresponding model will predict"]},{"cell_type":"code","metadata":{"id":"7dxhx2Dlw_kd"},"source":["# Order is: bh - bm - ph - pm - rh - rm - wh - wm\n","models_list = []\n","models_images_filenames = []\n","\n","# you can generate these checkpoints by running the specified notebooks in the proper way, or you can download them from this link:\n","# https://drive.google.com/drive/folders/1jvHfUNKOFcReSQLgm0IA3M79-PmXSSNc?usp=sharing\n","\n","path_model1 = 'checkpoint generated from unet_ADD_SKIP notebook, including only Bipbip Haricot'\n","path_model2 = 'checkpoint generated from UNET_VGG notebook, excluding Pead team'\n","path_model3 = 'checkpoint generated from unet_ADD_SKIP notebook, including only Pead Haricot'\n","path_model4 = 'checkpoint generated from UNET_PEAD_MAIS notebook, inlcuding only Pead Mais'\n","path_model5 = 'checkpoint generated from unet_ADD_SKIP notebook, including only Weedelec Haricot'\n","\n","# bipbip models\n","bh_model = tf.keras.models.load_model(path_model1, custom_objects={\"meanIoU\": meanIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,1.5], dtype=np.float32))}) \n","bm_model = tf.keras.models.load_model(path_model2, custom_objects={\"meanIoU\": meanIoU, \"cropIoU\": cropIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,2], dtype=np.float32))})\n","models_list.append(bh_model)\n","models_list.append(bm_model)\n","\n","# pead models\n","ph_model = tf.keras.models.load_model(path_model3, custom_objects={\"meanIoU\": meanIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,1.5], dtype=np.float32))})\n","pm_model = tf.keras.models.load_model(path_model4, custom_objects={\"meanIoU\": meanIoU, \"cropIoU\": cropIoU, \"weedIoU\": weedIoU})\n","models_list.append(ph_model)\n","models_list.append(pm_model)\n","\n","# roseau models\n","rh_model = tf.keras.models.load_model(path_model2, custom_objects={\"meanIoU\": meanIoU, \"cropIoU\": cropIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,2], dtype=np.float32))})\n","rm_model = tf.keras.models.load_model(path_model2, custom_objects={\"meanIoU\": meanIoU, \"cropIoU\": cropIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,2], dtype=np.float32))})\n","models_list.append(rh_model)\n","models_list.append(rm_model)\n","\n","# weedelec models\n","wh_model = tf.keras.models.load_model(path_model5, custom_objects={\"meanIoU\": meanIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,1.5], dtype=np.float32))})\n","wm_model = tf.keras.models.load_model(path_model2, custom_objects={\"meanIoU\": meanIoU, \"cropIoU\": cropIoU, \"loss\":weighted_categorical_crossentropy(np.asarray([0.5,1.,2], dtype=np.float32))})\n","models_list.append(wh_model)\n","models_list.append(wm_model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNLuSBjK5u9n"},"source":["The following cell builds the __models_images_filenames__ list containing all the lists of all filenames needed for prediction."]},{"cell_type":"code","metadata":{"id":"Y5F3eW-A0Vzu"},"source":["bool_arr_test = []\n","bool_arr_test.append([True, \"Bipbip\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Bipbip\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Roseau\", \"Haricot\", \".png\"])\n","bool_arr_test.append([True, \"Roseau\", \"Mais\", \".png\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Mais\", \".jpg\"])\n","\n","for i in range(0,8):\n","  team_images = []\n","  if bool_arr_test[i][0]:\n","    if i == 1 or i == 4 or i == 5 or i == 7:\n","      base_folder = os.path.join(cwd, \"Test_Final\")\n","    else:\n","      base_folder = os.path.join(cwd, dataset_version, \"Test_Final\")\n","    team = []\n","    crop = []\n","    names = []\n","    base_curr = os.path.join(base_folder, bool_arr_test[i][1], bool_arr_test[i][2])\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\n","    fn_images.sort()\n","    for entry in fn_images:\n","      names.append(entry[:-4])\n","    #print(names)\n","\n","    for j in range(0, len(fn_images)):\n","        team.append(bool_arr_test[i][1])\n","        crop.append(bool_arr_test[i][2])\n","    for index, value in enumerate(fn_images):\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\n","\n","    zipped_list = list(zip(fn_images, team, crop, names))\n","\n","    team_images += zipped_list\n","    models_images_filenames.append(team_images)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40Egk6586FMR"},"source":["## Start Testing\n","Once both the lists are ready we can start making predictions on the test images. This cell is specifically implemented for the kind of models that we are using, in case different models are loaded think of readapting it. This is due to the fact that some models need different preprocessing on the test images that they are predictinf but also different images since they may be tiled or not."]},{"cell_type":"code","metadata":{"id":"9i4JtxDi2WzK"},"source":["# json generation\n","# ---------------\n","\n","# this directory is where the submission.json file containing the results will be stored\n","exp_dir = '/content/drive/My Drive/challenge2/2_phase/Results'\n","\n","# size of the images used at training time for images that are not cropped\n","img_w = 1344\n","img_h = 960\n","\n","submission_dict = {}\n","\n","for i in range(0, 8):\n","  print(str(i))\n","  model = models_list[i]\n","  test_images = models_images_filenames[i]\n","  \n","  if i == 1 or i == 4 or i == 5 or i == 7: # models that need full images\n","    for entry in test_images:\n","      image = Image.open(entry[0])\n","      width, height = image.size\n","\n","      # resize image and create crops\n","      image = image.resize((img_w, img_h))\n","      img_arr = np.array(image)\n","\n","      out_sigmoid = model.predict(x=tf.keras.applications.vgg16.preprocess_input(tf.expand_dims(img_arr, 0)))\n","      resized_sigmoid = tf.image.resize(out_sigmoid, [height,width], method='nearest')\n","\n","      predicted_class = tf.argmax(resized_sigmoid, -1)\n","      predicted_class = predicted_class[0, ...]\n","\n","      img_name = entry[3]\n","\n","      mask_arr = np.array(predicted_class)\n","\n","      submission_dict[img_name] = {}\n","      submission_dict[img_name]['shape'] = mask_arr.shape\n","      submission_dict[img_name]['team'] = entry[1]\n","      submission_dict[img_name]['crop'] = entry[2]\n","      submission_dict[img_name]['segmentation'] = {}\n","\n","      # RLE encoding\n","      # crop\n","      rle_encoded_crop = rle_encode(mask_arr == 1)\n","      # weed\n","      rle_encoded_weed = rle_encode(mask_arr == 2)\n","\n","      submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","      submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","  else: # models that need cropped images\n","    for entry in test_images:\n","      image = Image.open(entry[0])\n","      width, height = image.size\n","\n","      # resize image and create crops\n","      image = image.resize(((width // tile_size)*tile_size, (height // tile_size)*tile_size))\n","      img_arr = np.array(image)\n","      image_crops = get_patches(img_arr, size=tile_size, stride=tile_size)\n","\n","      # prediction on each tile stacking each result\n","      tile_mask_list = []\n","      for j in range(len(image_crops)):\n","        tile_arr = image_crops[j]\n","        \n","        tile_arr = tile_arr * 1. / 255\n","        out_sigmoid = model.predict(x=tf.expand_dims(tile_arr, 0))\n","        \n","        predicted_class = tf.argmax(out_sigmoid, -1)\n","        predicted_class = predicted_class[0, ...]\n","\n","        tile_mask_list.append(np.array(tf.expand_dims(predicted_class, axis=-1)))\n","      \n","      mask_crops = np.stack(tile_mask_list)\n","\n","      # reconstruct and resize\n","      mask_reconstructed = reconstruct_from_patches(mask_crops, org_img_size=(image.height, image.width), stride=tile_size, size=tile_size)\n","      \n","      disegno = np.zeros((image.height, image.width , 3))\n","      disegno[np.where(mask_reconstructed[0,...,0] == 1)] = [255, 255, 255]\n","      disegno[np.where(mask_reconstructed[0,...,0] == 0)] = [0,0,0]\n","      disegno[np.where(mask_reconstructed[0,...,0] == 2)] = [216, 67, 82]\n","\n","      imm = Image.fromarray(np.uint8(disegno)).resize((width, height))\n","      mask_arr = np.array(imm)\n","\n","      new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n","\n","      new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n","      new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n","\n","      img_name = entry[3]\n","\n","      submission_dict[img_name] = {}\n","      submission_dict[img_name]['shape'] = new_mask_arr.shape\n","      submission_dict[img_name]['team'] = entry[1]\n","      submission_dict[img_name]['crop'] = entry[2]\n","      submission_dict[img_name]['segmentation'] = {}\n","\n","      # RLE encoding\n","      # crop\n","      rle_encoded_crop = rle_encode(new_mask_arr == 1)\n","      # weed\n","      rle_encoded_weed = rle_encode(new_mask_arr == 2)\n","\n","      submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","      submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","# Finally, save the results into the submission.json file\n","with open(os.path.join(exp_dir, \"submission.json\"), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4gka6XIrrsf"},"source":[""],"execution_count":null,"outputs":[]}]}