{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"UNET_VGG","provenance":[{"file_id":"1_yVdTV-ka6p8LU8AIay9RatbdBxqd6U4","timestamp":1608576770230},{"file_id":"1Jxt0bOMHHWecjWVmVtBUZptN751-vvcr","timestamp":1608574922325},{"file_id":"1zTgypN1dYtxbnsvQPEtURLgCqyfuuHN6","timestamp":1608400918342},{"file_id":"1-hcaDu6xYY_68sHm1vFbfoKSeqU-ykQ_","timestamp":1608300815469},{"file_id":"1KWAZ1z31U5ZxFAtw5yjB3EhMbUl9cuCj","timestamp":1607465245834}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LrO72wgdLHVU"},"source":["This is the architecture we use for:\r\n","\r\n","\r\n","*   Bipbip Mais\r\n","*   Roseau Haricot\r\n","*   Roseau Mais\r\n","*   Weedelec Mais\r\n","\r\n","It is very similar to U-Net. The encoder is the VGG16 architecture, unmodified, preloaded with 'imagenet' weights. The encoder has some upsampling blocks, made of transpose convolutions, convolutions, batch normalization and dropout layers.\r\n","\r\n","It has been trained on the whole dataset, with the exception of images belonging to \"Pead\", because they worsened the training.\r\n","\r\n","This notebook doesn't use tiles, but resizes images to 960 pixels in height and 1344 in width. Batch size has been set to 2 because higher values, with that image size, resulted in OOM error on Colab.\r\n","\r\n","\r\n","Note that this notebook is very similar to notebook unet_ADD_SKIP, so please refer to that one for commented code."]},{"cell_type":"code","metadata":{"id":"hb-zN1bbe687"},"source":[" from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ3XnuTye_vO"},"source":["import os\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","SEED = 1234\n","img_w = 1344\n","img_h = 960\n","bs = 2\n","lr = 1e-3\n","num_epochs = 100\n","patience = 7\n","num_classes = 3\n","\n","\n","# kfold\n","val_split_perc = 0.2\n","k = 5\n","enable_kfold = True\n","if not enable_kfold:\n","  k = int(1 / val_split_perc)\n","\n","\n","# boolean flags\n","# what you want to include in training phase\n","bool_arr = []\n","bool_arr.append([True, \"Bipbip\", \"Haricot\", \".jpg\"])\n","bool_arr.append([True, \"Bipbip\", \"Mais\", \".jpg\"])\n","bool_arr.append([False, \"Pead\", \"Haricot\", \".jpg\"])\n","bool_arr.append([False, \"Pead\", \"Mais\", \".jpg\"])\n","bool_arr.append([True, \"Roseau\", \"Haricot\", \".png\"])\n","bool_arr.append([True, \"Roseau\", \"Mais\", \".png\"])\n","bool_arr.append([True, \"Weedelec\", \"Haricot\", \".jpg\"])\n","bool_arr.append([True, \"Weedelec\", \"Mais\", \".jpg\"])\n","\n","# cosa includere nel test\n","# to generate a valid json though, you need to include all of these\n","bool_arr_test = []\n","bool_arr_test.append([True, \"Bipbip\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Bipbip\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Pead\", \"Mais\", \".jpg\"])\n","bool_arr_test.append([True, \"Roseau\", \"Haricot\", \".png\"])\n","bool_arr_test.append([True, \"Roseau\", \"Mais\", \".png\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Haricot\", \".jpg\"])\n","bool_arr_test.append([True, \"Weedelec\", \"Mais\", \".jpg\"])\n","\n","model_name = 'UNET_VGG'\n","\n","tf.random.set_seed(SEED) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WacDhwdYfA77"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x43-ArnVfRHA"},"source":["cwd = os.getcwd() # should be /content\n","\n","dataset_version = 'Development_Dataset'\n","\n","path_to_zip = '/content/drive/My\\ Drive/challenge2/dataset/' + dataset_version + '.zip'\n","\n","if not os.path.exists(os.path.join(cwd, dataset_version)):\n","    !unzip {path_to_zip}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcsG6HOa6-Fe"},"source":["import pandas as pd\r\n","\r\n","filenames_images = []\r\n","filenames_masks = []\r\n","\r\n","base_folder = os.path.join(cwd, dataset_version, \"Training\")\r\n","\r\n","for i in range(0,8):\r\n","  if bool_arr[i][0]:\r\n","    bf = []\r\n","    base_curr = os.path.join(base_folder, bool_arr[i][1], bool_arr[i][2])\r\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\r\n","    fn_images.sort()\r\n","    fn_masks = [x for x in os.listdir(os.path.join(base_curr, \"Masks\"))]\r\n","    fn_masks.sort()\r\n","\r\n","    for index, value in enumerate(fn_images):\r\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\r\n","\r\n","    for index, value in enumerate(fn_masks):\r\n","      fn_masks[index] = os.path.join(base_curr, \"Masks\", value)\r\n","\r\n","    filenames_images += fn_images\r\n","    filenames_masks += fn_masks\r\n","\r\n","data = pd.DataFrame(columns=[\"images\", \"masks\"])\r\n","data[\"images\"] = filenames_images\r\n","data[\"masks\"] = filenames_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9IVnZ-QyiWWE"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","prepr_func = tf.keras.applications.vgg16.preprocess_input\n","\n","\n","# Create training ImageDataGenerator object\n","# We need two different generators for images and corresponding masks\n","train_img_data_gen = ImageDataGenerator(rotation_range=30,\n","                                        width_shift_range=10,\n","                                        height_shift_range=10,\n","                                        zoom_range=0.3,\n","                                        horizontal_flip=True,\n","                                        vertical_flip=True,\n","                                        fill_mode='reflect',                                          \n","                                        preprocessing_function=prepr_func)\n","\n","train_mask_data_gen = ImageDataGenerator(rotation_range=30,\n","                                          width_shift_range=10,\n","                                          height_shift_range=10,\n","                                          zoom_range=0.3,\n","                                          horizontal_flip=True,\n","                                          vertical_flip=True,\n","                                          fill_mode='reflect')\n","\n","\n","# Create validation and test ImageDataGenerator objects\n","valid_img_data_gen = ImageDataGenerator(preprocessing_function=prepr_func)\n","valid_mask_data_gen = ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLCOOrE3Vr8F"},"source":["def prepare_target(x_, y_):\r\n","    return x_, (tf.cast(tf.reduce_any(y_ == 0, axis=-1, keepdims=True), tf.float32)*0 + \r\n","                tf.cast(tf.reduce_any(y_ == 124, axis=-1, keepdims=True), tf.float32)*0 + \r\n","                tf.cast(tf.reduce_any(y_ == 255, axis=-1, keepdims=True), tf.float32)*1 + \r\n","                tf.cast(tf.reduce_any(y_ == 67, axis=-1, keepdims=True), tf.float32)*2) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"svkxImmWtWTo"},"source":["from tensorflow.keras.layers import *\r\n","\r\n","def upsampleLayer(in_layer, concat_layer, num_filters):\r\n","\r\n","  upsample = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(in_layer) \r\n","  upsample = concatenate([concat_layer, upsample])\r\n","  conv = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(upsample)\r\n","  conv = BatchNormalization()(conv)\r\n","  conv = Dropout(0.2)(conv)\r\n","  conv = Conv2D(num_filters, (3, 3), activation='relu', padding='same')(conv)\r\n","  conv = BatchNormalization()(conv)\r\n","  return conv\r\n","\r\n","def create_model(num_classes):\r\n","\r\n","  inputs_1 = tf.keras.Input((img_h, img_w, 3))\r\n","\r\n","  base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=inputs_1)\r\n","\r\n","  base_model.trainable = False\r\n","\r\n","  # encoder\r\n","  c1 = base_model.get_layer(\"block1_conv2\").output \r\n","  c2 = base_model.get_layer(\"block2_conv2\").output \r\n","  c3 = base_model.get_layer(\"block3_conv3\").output \r\n","  c4 = base_model.get_layer(\"block4_conv3\").output \r\n","   \r\n","  # bottleneck\r\n","  c5 = base_model.get_layer(\"block5_conv3\").output \r\n","\r\n","  # decoder\r\n","  c6 = upsampleLayer(in_layer=c5, concat_layer=c4, num_filters=512)\r\n","  c7 = upsampleLayer(in_layer=c6, concat_layer=c3, num_filters=256)\r\n","  c8 = upsampleLayer(in_layer=c7, concat_layer=c2, num_filters=128)\r\n","  c9 = upsampleLayer(in_layer=c8, concat_layer=c1, num_filters=64)\r\n","\r\n","  outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c9)\r\n","\r\n","  model = tf.keras.Model(inputs=inputs_1, outputs=outputs)\r\n","  \r\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QC8x9QcJ3EvA"},"source":["from keras import backend as K\r\n","\r\n","def weighted_categorical_crossentropy(weights):\r\n","    weights = K.variable(weights)\r\n","\r\n","    def loss(y_true, y_pred):\r\n","      y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n","      # clip to prevent NaN's and Inf's\r\n","      y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\r\n","\r\n","      one_hot_encoding = (tf.cast(tf.reduce_any(y_true == 0, axis=-1, keepdims=True), tf.float32)*tf.constant([1.,0.,0.]) +\r\n","                          tf.cast(tf.reduce_any(y_true == 1, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,1.,0.]) +\r\n","                          tf.cast(tf.reduce_any(y_true == 2, axis=-1, keepdims=True), tf.float32)*tf.constant([0.,0.,1.]))\r\n","                    \r\n","      loss = one_hot_encoding * K.log(y_pred) * weights\r\n","      loss = -K.sum(loss, -1)\r\n","      return loss\r\n","    \r\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPfgobNhW9OE"},"source":["def meanIoU(y_true, y_pred):\r\n","    # get predicted class from softmax\r\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n","\r\n","    per_class_iou = []\r\n","\r\n","    for i in range(1,num_classes): # exclude the background class 0\r\n","      # Get prediction and target related to only a single class (i)\r\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n","      intersection = tf.reduce_sum(class_true * class_pred)\r\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n","    \r\n","      iou = (intersection + 1e-7) / (union + 1e-7)\r\n","      per_class_iou.append(iou)\r\n","\r\n","    return tf.reduce_mean(per_class_iou)\r\n","\r\n","def cropIoU(y_true, y_pred):\r\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n","    i = 1\r\n","    class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n","    class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n","    intersection = tf.reduce_sum(class_true * class_pred)\r\n","    union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n","    iou = (intersection + 1e-7) / (union + 1e-7)\r\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuwoEdYYaw05"},"source":["def display(display_list):\r\n","  plt.figure(figsize=(15, 15))\r\n","\r\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\r\n","\r\n","  for i in range(len(display_list)):\r\n","    plt.subplot(1, len(display_list), i+1)\r\n","    plt.title(title[i])\r\n","    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\r\n","    plt.axis('off')\r\n","  plt.show()\r\n","\r\n","def create_mask(pred_mask):\r\n","  pred_mask = tf.argmax(pred_mask, axis=-1)\r\n","  pred_mask = pred_mask[..., tf.newaxis]\r\n","  return pred_mask[0]\r\n","\r\n","def show_predictions(dataset=None, num=1):\r\n","  if dataset:\r\n","    for image, mask in dataset.take(num):\r\n","      pred_mask = model.predict(image)\r\n","      display([image[0], mask[0], create_mask(pred_mask)])\r\n","  else:\r\n","    display([train_sample_image, train_sample_mask,\r\n","             create_mask(model.predict(train_sample_image[tf.newaxis, ...]))])\r\n","    display([valid_sample_image, valid_sample_mask,\r\n","             create_mask(model.predict(valid_sample_image[tf.newaxis, ...]))])\r\n","    \r\n","class DisplayCallback(tf.keras.callbacks.Callback):\r\n","  def on_epoch_end(self, epoch, logs=None):\r\n","    #clear_output(wait=True)\r\n","    show_predictions()\r\n","    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGisKIPc93kf"},"source":["def get_callbacks(exp_dir):\r\n","  callbacks = []\r\n","\r\n","  callbacks.append(DisplayCallback())\r\n","\r\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","  if not os.path.exists(ckpt_dir):\r\n","      os.makedirs(ckpt_dir)\r\n","\r\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'),\r\n","                                                    monitor='val_loss',\r\n","                                                    mode='min', \r\n","                                                    save_weights_only=False,\r\n","                                                    save_best_only=True,\r\n","                                                    verbose=0)  \r\n","  callbacks.append(ckpt_callback)\r\n","\r\n","  # Early Stopping\r\n","  early_stop = True\r\n","  if early_stop:\r\n","      es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \r\n","                                                    patience=patience, \r\n","                                                    restore_best_weights=True)\r\n","      callbacks.append(es_callback)\r\n","\r\n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dFfT5FJXyRh"},"source":["from datetime import datetime\r\n","\r\n","exps_dir = '/content/drive/My Drive/challenge2/Teo/Models'\r\n","\r\n","if not os.path.exists(exps_dir):\r\n","    os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n","\r\n","if not os.path.exists(exp_dir):\r\n","    os.makedirs(exp_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iipYUXJ32KM8"},"source":["from keras.callbacks import LearningRateScheduler\r\n","def decay_schedule(epoch, lr):\r\n","    # decay by 0.1 every 7 epochs\r\n","    if (epoch % 7 == 0) and (epoch != 0):\r\n","        lr = lr * 0.1\r\n","    return lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5eNdD5J-pVn"},"source":["from sklearn.model_selection import KFold\r\n","\r\n","\r\n","kfold = KFold(n_splits=k, random_state=SEED, shuffle=True)\r\n","\r\n","loop_iteration = 0\r\n","\r\n","loss_arr = []\r\n","meanIoU_arr = []\r\n","\r\n","for train_index, val_index in kfold.split(X=data[\"images\"], y=data[\"masks\"]):\r\n","  training_data = data.iloc[train_index]\r\n","  validation_data = data.iloc[val_index]\r\n","\r\n","  #creation of the couple of generator for images and masks from training\r\n","  train_img_data_generator = train_img_data_gen.flow_from_dataframe(training_data,\r\n","                                                                    x_col = \"images\",\r\n","                                                                    shuffle = True,\r\n","                                                                    class_mode = None,\r\n","                                                                    target_size=(img_h, img_w),\r\n","                                                                    batch_size=bs,\r\n","                                                                    interpolation=\"nearest\",\r\n","                                                                    seed=SEED)\r\n","  train_mask_data_generator = train_mask_data_gen.flow_from_dataframe(training_data,\r\n","                                                                      x_col = \"masks\",\r\n","                                                                      shuffle = True,\r\n","                                                                      class_mode = None,\r\n","                                                                      target_size=(img_h, img_w),\r\n","                                                                      batch_size=bs,\r\n","                                                                      interpolation=\"nearest\",\r\n","                                                                      seed=SEED)\r\n","  train_gen = zip(train_img_data_generator, train_mask_data_generator)\r\n","\r\n","  #creation of the couple of generators for images and masks for validation  \r\n","  valid_img_data_generator = valid_img_data_gen.flow_from_dataframe(validation_data,\r\n","                                                       x_col = \"images\",\r\n","                                                       shuffle = True,\r\n","                                                       class_mode = None,\r\n","                                                       target_size=(img_h, img_w),\r\n","                                                       batch_size=bs,\r\n","                                                       interpolation=\"nearest\",\r\n","                                                       seed = SEED)\r\n","  valid_mask_data_generator = valid_mask_data_gen.flow_from_dataframe(validation_data, \r\n","                                                      x_col = \"masks\",\r\n","                                                      shuffle = True,\r\n","                                                      class_mode = None,\r\n","                                                      target_size=(img_h, img_w),\r\n","                                                      batch_size=bs,\r\n","                                                      interpolation=\"nearest\",\r\n","                                                      seed = SEED)\r\n","  valid_gen = zip(valid_img_data_generator, valid_mask_data_generator)\r\n","\r\n","  #######################\r\n","\r\n","  train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\r\n","  train_dataset = train_dataset.map(prepare_target)\r\n","  train_dataset = train_dataset.repeat()\r\n","\r\n","  valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \r\n","                                                output_types=(tf.float32, tf.float32),\r\n","                                                output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 3]))\r\n","  valid_dataset = valid_dataset.map(prepare_target)\r\n","  valid_dataset = valid_dataset.repeat()\r\n","\r\n","  for imageBatch, maskBatch in train_dataset.take(1):\r\n","    train_sample_image, train_sample_mask = imageBatch[0], maskBatch[0]\r\n","\r\n","  display([train_sample_image, train_sample_mask])\r\n","\r\n","  for imageBatch, maskBatch in valid_dataset.take(1):\r\n","    valid_sample_image, valid_sample_mask = imageBatch[0], maskBatch[0] \r\n","\r\n","\r\n","  #####################################################################################################################\r\n","\r\n","  model = create_model(num_classes)\r\n","  model.summary()\r\n","\r\n","  lr_scheduler = LearningRateScheduler(decay_schedule)\r\n","\r\n","  loss=weighted_categorical_crossentropy(np.asarray([0.5,1,2], dtype=np.float32))\r\n","\r\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","  metrics = ['accuracy', meanIoU, cropIoU]\r\n","\r\n","  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n","\r\n","  callbacks = get_callbacks(exp_dir)\r\n","  callbacks.append(lr_scheduler)\r\n","\r\n","  history = model.fit(x=train_dataset,\r\n","              epochs=num_epochs,\r\n","              steps_per_epoch=len(train_img_data_generator),\r\n","              validation_data=valid_dataset,\r\n","              validation_steps=len(valid_img_data_generator), \r\n","              callbacks=callbacks)\r\n"," \r\n","  minLoss = min(history.history['val_loss'])\r\n","  minLossIndex = history.history['val_loss'].index(minLoss)\r\n","  loss_arr.append(minLoss)\r\n","  meanIoU_arr.append(history.history['val_meanIoU'][minLossIndex])\r\n","  \r\n","  # print metrics to file\r\n","  with open(os.path.join(exp_dir, 'historySplit' + str(loop_iteration) + '.txt'), 'w') as f:\r\n","    for key in history.history.keys():\r\n","      print(str(key), file=f)\r\n","      print(history.history[key], file=f)\r\n","\r\n","  if not enable_kfold:\r\n","    break\r\n","  \r\n","  loop_iteration += 1\r\n","\r\n","with open(os.path.join(exp_dir, 'cv_results' + '.txt'), 'w') as f2:\r\n","    print(\"avg loss = {}\".format(np.mean(loss_arr)), file=f2)\r\n","    print(\"avg meanIoU = {}\".format(np.mean(meanIoU_arr)), file=f2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxBU6H5NDWkL"},"source":["test_images = []\r\n","\r\n","base_folder = os.path.join(cwd, dataset_version, \"Test_Dev\")\r\n","\r\n","for i in range(0,8):\r\n","  if bool_arr_test[i][0]:\r\n","    team = []\r\n","    crop = []\r\n","    names = []\r\n","    base_curr = os.path.join(base_folder, bool_arr_test[i][1], bool_arr_test[i][2])\r\n","    fn_images = [x for x in os.listdir(os.path.join(base_curr, \"Images\"))]\r\n","    fn_images.sort()\r\n","    for entry in fn_images:\r\n","      names.append(entry[:-4])\r\n","\r\n","    for j in range(0, len(fn_images)):\r\n","        team.append(bool_arr_test[i][1])\r\n","        crop.append(bool_arr_test[i][2])\r\n","    for index, value in enumerate(fn_images):\r\n","      fn_images[index] = os.path.join(base_curr, \"Images\", value)\r\n","\r\n","    zipped_list = list(zip(fn_images, team, crop, names))\r\n","\r\n","    test_images += zipped_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6NuMFELiXkj"},"source":["def rle_encode(img):\r\n","    '''\r\n","    img: numpy array, 1 - foreground, 0 - background\r\n","    Returns run length as string formatted\r\n","    '''\r\n","    pixels = img.flatten()\r\n","    pixels = np.concatenate([[0], pixels, [0]])\r\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n","    runs[1::2] -= runs[::2]\r\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qq5Gi48gzV4t"},"source":["# json generation\r\n","\r\n","import json\r\n","from PIL import Image\r\n","\r\n","\r\n","submission_dict = {}\r\n","\r\n","for entry in test_images:\r\n","  \r\n","  image = Image.open(entry[0])\r\n","  width, height = image.size\r\n","\r\n","  image = image.resize((img_w, img_h))\r\n","  img_arr = np.array(image)\r\n","\r\n","  out_sigmoid = model.predict(x=tf.keras.applications.vgg16.preprocess_input(tf.expand_dims(img_arr, 0)))\r\n","  resized_sigmoid = tf.image.resize(out_sigmoid, [height,width], method='nearest')\r\n","\r\n","  predicted_class = tf.argmax(resized_sigmoid, -1)\r\n","  predicted_class = predicted_class[0, ...]\r\n","\r\n","  img_name = entry[3]\r\n","\r\n","  mask_arr = np.array(predicted_class)\r\n","\r\n","  submission_dict[img_name] = {}\r\n","  submission_dict[img_name]['shape'] = mask_arr.shape\r\n","  submission_dict[img_name]['team'] = entry[1]\r\n","  submission_dict[img_name]['crop'] = entry[2]\r\n","  submission_dict[img_name]['segmentation'] = {}\r\n","\r\n","  # RLE encoding\r\n","  # crop\r\n","  rle_encoded_crop = rle_encode(mask_arr == 1)\r\n","  # weed\r\n","  rle_encoded_weed = rle_encode(mask_arr == 2)\r\n","\r\n","  submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\r\n","  submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\r\n","\r\n","\r\n","# Finally, save the results into the submission.json file\r\n","with open(os.path.join(exp_dir, \"submission.json\"), 'w') as f:\r\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}