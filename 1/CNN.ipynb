{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aJUXMhGOGJ8i"},"source":["# Convolutional Neural Network\n","The following notebook implements the best convolutional neural network we were able to achieve. Images are rescaled to 256x256 and data augmentation is applied on the training set with the best hyperparameters obtained after a parameter tuning study. The following table summarizes the hyperparameters of the network:\n","\n","| Hyperparameter <img width=100/> | Value <img width=50/>|\n","|:-|:-:|\n","| batch_size <img width=100/> | 16 <img width=50/> |\n","| img_h <img width=100/> | 256  <img width=50/> |\n","| img_w <img width=100/> | 256 <img width=50/> |\n","| start_f <img width=100/> | 64 <img width=50/> |\n","| depth <img width=100/> | 5 <img width=50/> |\n","| learning_rate  <img width=100/> | 0.0001 <img width=50/> |\n","\n","The model obtained is compiled with __Adam__ optimizer and the accuracy __metric__. Finally, the techniques used in order to face __overfitting__ are:\n","* __Early Stopping__: called with ```(monitor='val_loss', patience=5)```\n","* __Dropout__: only on the last dense layer with probability equal to 0.1 "]},{"cell_type":"code","metadata":{"id":"dJQU7_nAgPj7"},"source":["import os\n","import tensorflow as tf\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZ3rN2NsP2Bx"},"source":["## Directories Initialization\n","The notebook can be run independently referring to the __base__ folder created in the following cells. In colab, once mounted drive and unzipped the dataset, we can continue the whole experiment in the base directory only. Remember to assign to the ```zip_path``` variable the path to your MaskDataset.zip."]},{"cell_type":"code","metadata":{"id":"ANCWk-QUQiM3"},"source":["# DIRECTORIES\n","zip_path = 'INSERT_HERE_ZIP_PATH' \n","base_dir = '/content/base'\n","if not os.path.exists(base_dir):\n","  os.makedirs(base_dir)\n","\n","os.chdir(base_dir)\n","cwd = os.getcwd()\n","\n","# checkpointing and results directories\n","exps_dir = os.path.join(cwd, 'Checkpoints')\n","if not os.path.exists(exps_dir):\n","  os.makedirs(exps_dir)\n","\n","res_dir = os.path.join(cwd, 'Results')\n","if not os.path.exists(res_dir):\n","  os.makedirs(res_dir)\n","\n","# dataset directories\n","dataset_dir = os.path.join(cwd, 'MaskDataset')\n","training_dir = os.path.join(dataset_dir, 'train')\n","validation_dir = os.path.join(dataset_dir, 'val')\n","test_dir = os.path.join(dataset_dir, 'test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYN1g9sQSMJG"},"source":["### Directory Structure\n","\n","Now we have to mount drive and unzip the dataset. Finally the directories will be structured in the following way:\n","    \n","    - base/\n","      - Checkpoints/\n","      - Results/\n","      - MaskDataset/\n","          - test/\n","              - img1, img2, …, imgN\n","          - train/\n","              - 0_none/\n","                  - img1, img2, …, imgN\n","              - 1_all/\n","                  - img1, img2, …, imgN\n","              - 2_some/ \n","                  - img1, img2, ... , imgN\n","          - val/\n","              - 0_none/\n","                  - img1, img2, …, imgN\n","              - 1_all/\n","                  - img1, img2, …, imgN\n","              - 2_some/ \n","                  - img1, img2, ... , imgN\n","            "]},{"cell_type":"code","metadata":{"id":"jEJUV2Njd3Xr"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGb5NYRNeKFB"},"source":["# MaskDataset unzipping, provide the path of MaskDataset.zip in zip_path\n","!unzip {zip_path}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3rA1nL2WNE8"},"source":["## Network implementation\n","We now have to realize the network, first we decide the hyperparameters, apply data preprocessing and build the data generators to train the network. Then, we encode the architecture, compile the model and finally train it."]},{"cell_type":"markdown","metadata":{"id":"V_YuneTEXD8q"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"eI41eAr4Pydu"},"source":["# SEED setting\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","\n","# hyperparameters\n","bs = 16\n","img_h = 256\n","img_w = 256\n","start_f = 64\n","depth = 5\n","lr = 1e-4\n","\n","# model features\n","num_classes = 3\n","model_name = 'CNN_5'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cEeYIbItXuQi"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"id":"58kInjvUrNzI"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# data training is augmented\n","train_data_gen = ImageDataGenerator(rotation_range=10,\n","                                    width_shift_range=10,\n","                                    height_shift_range=10,\n","                                    zoom_range=0.3,\n","                                    shear_range=0.3,\n","                                    horizontal_flip=True,\n","                                    vertical_flip=False,\n","                                    fill_mode='constant',\n","                                    cval=0,\n","                                    rescale=1./255)\n","\n","# validation just rescaled\n","valid_data_gen = ImageDataGenerator(rescale=1/255.)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvORA3VSYGkj"},"source":["### Data Management"]},{"cell_type":"code","metadata":{"id":"2zflvzr_sqQE"},"source":["# GENERATORS\n","classes = [\n","     '0_none',\n","     '1_all',\n","     '2_some'      \n","]\n","\n","# Training\n","train_gen = train_data_gen.flow_from_directory(training_dir,\n","                                               batch_size=bs,\n","                                               classes=classes,\n","                                               class_mode='categorical',\n","                                               shuffle=True,\n","                                               seed=SEED)\n","\n","# Validation\n","valid_gen = valid_data_gen.flow_from_directory(validation_dir,\n","                                               batch_size=bs, \n","                                               classes=classes,\n","                                               class_mode='categorical',\n","                                               shuffle=False,\n","                                               seed=SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qSP-oRE3EZA"},"source":["# DATASET OBJECTS\n","\n","# Training\n","train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n","\n","train_dataset = train_dataset.repeat()\n","\n","# Validation\n","valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UWifUXUpYZ71"},"source":["### Model Architecture\n","The architecture of the Convolutional Neural Network is composed of the convolutional part of depth 5 followed by two dense layers where only the last one has dropout applied on it. In the end we find the last dense layer with a number of units corresponding to our classes, which allows to make the classification."]},{"cell_type":"code","metadata":{"id":"xWSQ1a4Q46Z7"},"source":["# ARCHITECTURE\n","model = tf.keras.Sequential()\n","\n","# Convolutional Part\n","for i in range(depth):\n","  if i == 0:\n","    input_shape = [img_h, img_w, 3]\n","  else:\n","    input_shape=[None]\n","\n","  model.add(tf.keras.layers.Conv2D(filters=start_f, \n","                                   kernel_size=(3, 3),\n","                                   strides=(1, 1),\n","                                   padding='same',\n","                                   input_shape=input_shape))\n","  model.add(tf.keras.layers.ReLU())\n","  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n","\n","  start_f *= 2\n","\n","# Bottom dense Part\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n","model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwuXsdO_aO4Z"},"source":["### Model Compile"]},{"cell_type":"code","metadata":{"id":"ZYoTj-zo78j6"},"source":["# loss function\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","# optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","# metrics\n","metrics = ['accuracy']\n","\n","# compile model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oniFlAkMaWt_"},"source":["### Training with Callbacks\n","The following callbacks are used during training for model checkpointing, data visualization and early stopping. "]},{"cell_type":"code","metadata":{"id":"-BREu9hG8Ejj"},"source":["from datetime import datetime\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","# Experiments dir\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n","                                                   save_weights_only=True)\n","callbacks.append(ckpt_callback)\n","\n","# Visualize Learning on Tensorboard\n","# ---------------------------------\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)  # if 1 shows weights histograms\n","callbacks.append(tb_callback)\n","\n","# Early Stopping\n","# --------------\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JvSMZJ7pctfr"},"source":["### Model Training\n","We can now visualize the model by inspecting its summary and finally call the fit method in order to start the training."]},{"cell_type":"code","metadata":{"id":"yAS4cQDWdaqT"},"source":["# Model Summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2f8kRBlOfIfI"},"source":["# TRAINING\n","model.fit(x=train_dataset,\n","          epochs=100,\n","          steps_per_epoch=len(train_gen),\n","          validation_data=valid_dataset,\n","          validation_steps=len(valid_gen), \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0G6r8wjgdyyD"},"source":["## Model Testing\n","Once the model is trained we can test it by calling the predict method on the images contained in the test dataset. In the end we build the .csv file used for the submissions."]},{"cell_type":"code","metadata":{"id":"dhGl93SHajw4"},"source":["def create_csv(results, model_name):\n","    csv_fname = model_name + '_results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(res_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUh-4zv7YBVh"},"source":["from PIL import Image\n","\n","image_filenames = next(os.walk(test_dir + '/test'))[2]\n","\n","results = {}\n","for image_name in image_filenames:\n","  img = Image.open(test_dir + '/test/' + image_name).convert('RGB')\n","  img = img.resize((img_h, img_w))\n","\n","  img_array = np.array(img)\n","  img_array = img_array * 1. / 255\n","  img_array = np.expand_dims(img_array, 0)\n","\n","  predictions = model.predict(img_array)\n","  predicted_class = np.argmax(predictions, axis=-1)\n","\n","  predicted_class = predicted_class[0]\n","  results[image_name] = predicted_class\n","\n","create_csv(results, model_name)"],"execution_count":null,"outputs":[]}]}